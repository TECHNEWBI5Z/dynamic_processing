{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db0613d-248e-426b-adc7-59cb98bfcc0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Mounting of ADLS GEN 2 Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a716254-01ca-4560-a04e-8a323f3d9a5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the already_mounted variable\n",
    "already_mounted = False\n",
    "\n",
    "# Check if the mount point already exists\n",
    "for x in dbutils.fs.mounts():\n",
    "    if x.mountPoint == \"/mnt/sales\":\n",
    "        already_mounted = True\n",
    "        break\n",
    "\n",
    "# Print whether it's already mounted\n",
    "print(\"Already mounted:\", already_mounted)\n",
    "\n",
    "# Check if already mounted\n",
    "if not already_mounted:\n",
    "    # Define the storage account name and key\n",
    "    storage_account_name = \"faizanstore\"\n",
    "    storage_account_key = secrete_adls_key\n",
    "    container_name = \"data-import\"\n",
    "    mount_point = \"/mnt/sales\"\n",
    "\n",
    "    # Set the configurations\n",
    "    configs = {\n",
    "        \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name): storage_account_key\n",
    "    }\n",
    "\n",
    "    # Mount the storage\n",
    "    dbutils.fs.mount(\n",
    "        source=\"wasbs://{}@{}.blob.core.windows.net/\".format(container_name, storage_account_name),\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=configs\n",
    "    )\n",
    "    already_mounted = True\n",
    "    print(\"Mounting done successfully\")\n",
    "else:\n",
    "    print(\"It is already mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b50617a2-4f68-47a5-8633-ca369e692073",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fd4e2b2-844e-4ecd-a350-6d8ddd3ce258",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the database name\n",
    "database_name = \"growth_lakehouse\"\n",
    "\n",
    "# Check if the database exists\n",
    "databases = spark.sql(\"SHOW DATABASES\").collect()\n",
    "database_exists = any(db.databaseName == database_name for db in databases)\n",
    "\n",
    "# Create the database if it doesn't exist\n",
    "if not database_exists:\n",
    "    spark.sql(f\"CREATE DATABASE {database_name}\")\n",
    "    print(f\"Database '{database_name}' created.\")\n",
    "else:\n",
    "    print(f\"Database '{database_name}' already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab7277b-8df1-47dc-837f-15b9615d6f44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Drop Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d96b228-650c-409b-8a5d-d6cba8ed47a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the database name\n",
    "database_name = \"growth_lakehouse\"\n",
    "\n",
    "# Get the list of tables in the database\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {database_name}\")\n",
    "\n",
    "# Drop each table in the database\n",
    "for row in tables_df.collect():\n",
    "    table_name = row['tableName']\n",
    "    print(f\"Dropping table: {table_name}\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {database_name}.{table_name}\")\n",
    "\n",
    "# Drop the database\n",
    "print(f\"Dropping database: {database_name}\")\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {database_name} CASCADE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2f5c92-9a89-4440-9108-7ccc8aa12518",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Create Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "974ebcb6-48fc-4936-a644-882688137c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_directory_if_not_exists(directory_path):\n",
    "    try:\n",
    "        # Check if the directory exists by listing its contents\n",
    "        dbutils.fs.ls(directory_path)\n",
    "        print(f\"Directory {directory_path} already exists.\")\n",
    "    except Exception as e:\n",
    "        # If an exception is raised, it means the directory does not exist\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(f\"Directory {directory_path} does not exist. Creating it.\")\n",
    "            dbutils.fs.mkdirs(directory_path)\n",
    "            print(\"bronze layer folder created\")\n",
    "        else:\n",
    "            # Raise the exception if it's a different error\n",
    "            raise e\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"/mnt/lakehouse/bronze_layer\"\n",
    "create_directory_if_not_exists(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60c417a3-f41d-4006-a742-78370b515afd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Flatten the JSON File Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02e0279b-7893-4d9f-8c03-2299d987888d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def child_struct(nested_df):\n",
    "    # Creating python set to store dataframe metadata\n",
    "    list_schema = [((), nested_df)]\n",
    "    \n",
    "    # Creating empty python list for final flattened columns\n",
    "    flat_columns = []\n",
    "\n",
    "    # Looping until there are no more schemas to process\n",
    "    while len(list_schema) > 0:\n",
    "        # Removing the latest or recently added item (dataframe schema) and returning it into the df variable\n",
    "        parents, df = list_schema.pop()\n",
    "        \n",
    "        # Creating columns for non-struct fields\n",
    "        flat_cols = [\n",
    "            col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "        \n",
    "        # Identifying columns that are of struct type\n",
    "        struct_cols = [c[0] for c in df.dtypes if c[1][:6] == \"struct\"]\n",
    "        \n",
    "        # Adding flat columns to the flat_columns list\n",
    "        flat_columns.extend(flat_cols)\n",
    "        \n",
    "        # Reading nested columns and appending into the stack list\n",
    "        for i in struct_cols:\n",
    "            projected_df = df.select(i + \".*\")\n",
    "            list_schema.append((parents + (i,), projected_df))\n",
    "    \n",
    "    # Returning the flattened DataFrame with all columns\n",
    "    return nested_df.select(flat_columns)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "def master_array(df: DataFrame) -> DataFrame:\n",
    "    # Get initial list of array columns\n",
    "    array_cols = [c[0] for c in df.dtypes if c[1].startswith(\"array\")]\n",
    "    \n",
    "    while len(array_cols) > 0:\n",
    "        for c in array_cols:\n",
    "            df = df.withColumn(c, explode_outer(col(c)))\n",
    "        \n",
    "        # Update the list of array columns after the explosion\n",
    "        # Assume child_struct is a function that handles additional struct flattening\n",
    "        df = child_struct(df)\n",
    "        \n",
    "        # Get the updated list of array columns\n",
    "        array_cols = [c[0] for c in df.dtypes if c[1].startswith(\"array\")]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "final_output = master_array(df)\n",
    "display(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c5b432f-28f4-49fa-95a3-91e67836aecf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Metadata backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28ebb8b0-a639-472d-ac93-d12f1f2dd6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/temp/ddls/\")\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "# Function to create metadata DDLs for all tables in a database\n",
    "def create_metadata_DDL(database):\n",
    "    # List all tables in the given database\n",
    "    all_tables = spark.catalog.listTables(database)\n",
    "    \n",
    "    # Open a file to write the DDL statements\n",
    "    with open(\"/temp/ddls/bkp_{}.sql\".format(database), \"w\") as f:\n",
    "        for t in all_tables:\n",
    "            # Generate the DDL statement for each table\n",
    "            ddls = spark.sql(\"SHOW CREATE TABLE {}.{};\".format(database, t.name))\n",
    "            # Write the DDL to the file\n",
    "            f.write(ddls.first()[0])\n",
    "            f.write(\";\\n\")  # Add a semicolon and a newline after each DDL statement\n",
    "\n",
    "# Get a list of all databases\n",
    "DB_List = [db.databaseName for db in spark.sql(\"SHOW DATABASES\").collect()]\n",
    "\n",
    "# Use threading to create DDLs in parallel for each database\n",
    "processes = ThreadPool(4)\n",
    "processes.map(create_metadata_DDL, DB_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c25758-ec9c-467f-86c0-ba70c9c64365",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Capture File Name through ADF parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf6e090-c297-4cfa-b51f-fc2b8483141d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### To get ADF pipeline parameter into databricks notebook\n",
    "##### There is function with support to get filename through storage event trigger.\n",
    "#### @triggerBody().fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1027e2ca-defc-4c6c-bb74-98e7882af426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filename = dbutils.widgets.get('filename')\n",
    "df = spark.read.option(\"permissive\", \"true\").option(\"badRecordsPath\", \"<file_path>/badRecords/{}\".format(filename)).csv(f\"<file_path>/{filename}\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f766c22d-de6c-4344-8733-abdba4d8f3e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 8. Dynamic SQL Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44cba830-955b-4806-ac79-990f912561f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"my_table\"\n",
    "filter_condition = \"column1 = 'value'\"\n",
    "\n",
    "query = f\"SELECT * FROM {table_name} WHERE {filter_condition}\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e57e1a32-4f88-4bc3-be67-fcafd719a35e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 9. Integration with External Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5bbd3a8-55c3-4754-961e-a88d8b01a6ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.example.com/data\")\n",
    "if response.status_code == 200:\n",
    "    print(\"Data fetched successfully\")\n",
    "    data = response.json()\n",
    "    # Process the data\n",
    "else:\n",
    "    print(\"Failed to fetch data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d13494-ab8f-4f1c-847b-b56fc2ad9e82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 10. Dynamic Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24daf428-b7b2-44f5-ad19-2b57cc7af60f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_source = \"s3://my-bucket/data/\"\n",
    "\n",
    "# Load different datasets based on some condition\n",
    "if input_value == \"dataset1\":\n",
    "    df = spark.read.csv(data_source + \"dataset1.csv\")\n",
    "elif input_value == \"dataset2\":\n",
    "    df = spark.read.parquet(data_source + \"dataset2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac33972-39c4-446f-a152-ccc3ce39fa67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 11. Dynamic Partitioning and Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d2f04d-e8b9-4034-bdc5-e5fbad01fc64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "partition_column = \"date\"\n",
    "bucket_column = \"user_id\"\n",
    "\n",
    "df.write.partitionBy(partition_column).bucketBy(10, bucket_column).saveAsTable(\"my_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "024417a3-0a00-466e-a267-a711f77fb038",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 12. Dynamic Configuration of Spark Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6e5099-5f3b-4cc0-bdcf-c07bd6beeb0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dynamically adjust shuffle partitions based on input size\n",
    "input_size_gb = 50  # This could be calculated or passed as an argument\n",
    "shuffle_partitions = max(10, input_size_gb * 2)\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", shuffle_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165aee2a-0b3b-4b63-8807-df251af3acb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 13. Parameter-Driven Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c2c0fb7-cb3b-4ec9-91ca-7cc9ec901e91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"stage\", \"all\", \"Pipeline Stage\")\n",
    "stage = dbutils.widgets.get(\"stage\")\n",
    "\n",
    "if stage == \"ingest\" or stage == \"all\":\n",
    "    # Ingest data\n",
    "    print(\"Ingesting data...\")\n",
    "if stage == \"transform\" or stage == \"all\":\n",
    "    # Transform data\n",
    "    print(\"Transforming data...\")\n",
    "if stage == \"load\" or stage == \"all\":\n",
    "    # Load data\n",
    "    print(\"Loading data into the warehouse...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e68a3ce5-adca-4cbd-bde3-f380c3a80386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 14. Dynamic Error Handling and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d31726d-3b2c-44fd-be47-896f9fd7c722",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    df = spark.read.csv(\"data.csv\")\n",
    "    df.show()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error processing data: {str(e)}\")\n",
    "    # Implement dynamic retry logic or alternative processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b7ce362-6309-45a2-9356-e07526d59d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 15. Dynamic Notification and Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20ccc3d0-d003-4eed-a26f-134cebc5bd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "if df.filter(col(\"error_flag\") == True).count() > 0:\n",
    "    # Trigger an alert\n",
    "    dbutils.notebook.exit(\"Error detected in data processing\")\n",
    "else:\n",
    "    print(\"Data processing started\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dynamic_expression",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
